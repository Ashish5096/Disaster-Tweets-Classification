{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset: https://www.kaggle.com/competitions/nlp-getting-started/overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing NaN values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this code we will only consider keywords and target column. Both the columns do not have any NA values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[['text','target']]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7521, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop_duplicates()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis on Target column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a goooooooaaaaaal!!!!!!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'][19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps To be taken to clean the text\n",
    "- Remove punctutaions #.,-?! from text containing it. \n",
    "- Remove numerics\n",
    "- convert text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    \n",
    "    for i in range(0, len(text)):\n",
    "        \n",
    "        # Remove urls\n",
    "        text[i] = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"\",text[i])\n",
    "        \n",
    "        # Remove Hashtags\n",
    "        text[i] = re.sub('#(\\w+)',\"\",text[i])\n",
    "        \n",
    "        # Remove @tags\n",
    "        text[i] = re.sub('@(\\w+)',\"\",text[i])\n",
    "        \n",
    "        # Remove &tags\n",
    "        text[i] = re.sub('&(\\w+)',\"\",text[i])\n",
    "        \n",
    "        # Remove Non ASCII characters\n",
    "        text[i] = re.sub(r'[^\\x00-\\x7F]+',' ', text[i])\n",
    "        \n",
    "        # Remove numbers [0-9]\n",
    "        text[i] = re.sub('[0-9]',\"\",text[i])\n",
    "        \n",
    "        # Split words seprated with -\n",
    "        text[i] = re.sub('-',\" \",text[i])\n",
    "        \n",
    "        text[i] = re.sub('[#.,;?!)/^(}{:%+=$*\\|~_]',' ',text[i])\n",
    "        \n",
    "        # Remove square brackets\n",
    "        text[i] = re.sub('[\\[\\]]',' ',text[i])\n",
    "        \n",
    "        # Remove \\n from text\n",
    "        text[i] = re.sub('\\n',' ',text[i])\n",
    "        \n",
    "        text[i] = re.sub('[\\']',' ',text[i])\n",
    "        \n",
    "        # Remove extra space\n",
    "        text[i] = re.sub(' +', ' ', text[i])\n",
    "        \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data of training set\n",
    "\n",
    "train_df = train_df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean training data\n",
    "\n",
    "data_x = train_df['text'].tolist()\n",
    "data_y = train_df['target'].tolist()\n",
    "\n",
    "data_x = clean_data(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "7613\n"
     ]
    }
   ],
   "source": [
    "print(len(data_x))\n",
    "print(len(data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and validation sets\n",
    "\n",
    "train_len = int(0.8 * len(data_x))\n",
    "\n",
    "train_x = np.array(data_x[0:train_len])\n",
    "train_y = np.array(data_y[0:train_len])\n",
    "\n",
    "val_x = np.array(data_x[train_len:])\n",
    "val_y = np.array(data_y[train_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data  6090 6090\n",
      "Length of validation data 1523 1523\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of training data \", len(train_x), len(train_y))\n",
    "print(\"Length of validation data\", len(val_x), len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Size\n",
    "\n",
    "voc_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a tokenizer\n",
    "\n",
    "#tokenizer = Tokenizer(num_words=voc_size,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True, split=' ')\n",
    "tokenizer = Tokenizer(num_words=voc_size,filters=\"\",lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090, 33)\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# Padding Sequences so that the length of each sequence is same\n",
    "# Setting maxlen = None, so that sequences will be padded to the length of the longest individual sequence\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_x)\n",
    "train_pad_seq = pad_sequences(train_seq, padding='post', maxlen=None)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = train_pad_seq.shape[1]\n",
    "\n",
    "print(train_pad_seq.shape)\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1523, 33)\n"
     ]
    }
   ],
   "source": [
    "# Converting val data to sequences\n",
    "\n",
    "val_seq = tokenizer.texts_to_sequences(val_x)\n",
    "val_pad_seq = pad_sequences(val_seq, padding='post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(val_pad_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11351\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11352\n"
     ]
    }
   ],
   "source": [
    "voc_size = len(tokenizer.word_index)+1\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of Google Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding,Bidirectional, Input, LSTM, Dropout, concatenate\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Lambda, Reshape, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "google_embeddings_model = keyedvectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2453"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The index of vector for a token which is present in the vocabulary\n",
    "\n",
    "rock_idx = google_embeddings_model.key_to_index[\"rock\"]\n",
    "rock_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# The embedded vector for specific token which is present in the vocabulary\n",
    "\n",
    "vector1 = google_embeddings_model.get_vector(\"goal\",norm=True)\n",
    "vector2 = google_embeddings_model.get_vector(\"goal\")\n",
    "\n",
    "print(vector1.shape)\n",
    "print(vector2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14448193\n",
      "0.14587118\n"
     ]
    }
   ],
   "source": [
    "print(min(vector1))\n",
    "print(max(vector1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.40625\n",
      "0.41015625\n"
     ]
    }
   ],
   "source": [
    "print(min(vector2))\n",
    "print(max(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matrix(model, vocab):\n",
    "    \n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weight_matrix = np.zeros((vocab_size, 300))\n",
    "    \n",
    "    for word, i in vocab.items():\n",
    "        try:\n",
    "            weight_matrix[i] = model.get_vector(word,norm=True)\n",
    "        except:\n",
    "            weight_matrix[i] = np.random.uniform(low=-0.1, high=0.1, size=300)\n",
    "            \n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = get_weight_matrix(google_embeddings_model,tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11352, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Architecture based on : https://arxiv.org/abs/1408.5882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =300\n",
    "filter_sizes=[3, 4, 5]\n",
    "num_filters=[100, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, voc_size, embedding_dim, num_classes, fine_tune):\n",
    " \n",
    "    embedding_layer = Embedding(voc_size,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=fine_tune)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    \n",
    "    convs = []\n",
    "    filter_sizes=[3, 4, 5]\n",
    "    num_filters=[100, 100, 100]\n",
    "    \n",
    "    for i in range(0,len(filter_sizes)):\n",
    "        l_conv = Conv1D(filters=num_filters[i], kernel_size=filter_sizes[i], activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        \n",
    "        convs.append(l_pool)\n",
    "        \n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    x = Dropout(0.5)(l_merge)\n",
    "    x = Dense(np.sum(num_filters), activation='relu')(x)\n",
    "    \n",
    "    preds = Dense((num_classes-1), activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 33, 300)      3405600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 31, 100)      90100       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 30, 100)      120100      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 29, 100)      150100      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 100)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 300)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300)          90300       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            301         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 3,856,501\n",
      "Trainable params: 450,901\n",
      "Non-trainable params: 3,405,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 23:43:03.801854: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-13 23:43:03.829340: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(embedding_vectors, MAX_SEQUENCE_LENGTH,  voc_size, 300,2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 23:43:12.482658: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - 4s 19ms/step - loss: 0.5216 - acc: 0.7484 - val_loss: 0.4297 - val_acc: 0.8129\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 4s 19ms/step - loss: 0.4125 - acc: 0.8159 - val_loss: 0.4748 - val_acc: 0.7728\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 3s 18ms/step - loss: 0.3454 - acc: 0.8562 - val_loss: 0.4130 - val_acc: 0.8221\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 4s 19ms/step - loss: 0.2792 - acc: 0.8857 - val_loss: 0.4736 - val_acc: 0.8168\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 4s 21ms/step - loss: 0.2316 - acc: 0.9141 - val_loss: 0.5068 - val_acc: 0.7879\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 4s 23ms/step - loss: 0.2016 - acc: 0.9289 - val_loss: 0.4875 - val_acc: 0.8109\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 4s 21ms/step - loss: 0.1735 - acc: 0.9394 - val_loss: 0.5292 - val_acc: 0.8253\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 4s 22ms/step - loss: 0.1518 - acc: 0.9506 - val_loss: 0.5210 - val_acc: 0.8221\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 4s 21ms/step - loss: 0.1431 - acc: 0.9530 - val_loss: 0.5165 - val_acc: 0.8234\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 4s 22ms/step - loss: 0.1214 - acc: 0.9593 - val_loss: 0.5876 - val_acc: 0.8122\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "hist = model.fit(x=train_pad_seq, y=train_y, epochs=num_epochs, validation_data=(val_pad_seq,val_y),workers=4,\n",
    "                 use_multiprocessing=True,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
